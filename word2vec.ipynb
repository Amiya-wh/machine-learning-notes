{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec笔记\n",
    "\n",
    "学习word2vec的skip-gram实现，除了skip-gram模型还有CBOW模型。\n",
    "Skip-gram模式是根据中间词，预测前后词，CBOW模型刚好相反，根据前后的词，预测中间词。\n",
    "\n",
    "那么什么是**中间词**呢？什么样的词才叫做**前后词**呢？\n",
    "\n",
    "首先，我们需要定义一个窗口大小，在窗口里面的词，我们才有中间词和前后词的定义。一般这个窗口大小在5-10之间。\n",
    "举个例子，我们设置窗口大小（window size）为2：\n",
    "```bash\n",
    "|The|quick|brown|fox|jump|\n",
    "```\n",
    "那么，`brown`就是我们的中间词，`The`、`quick`、`fox`、`jump`就是前后词。\n",
    "\n",
    "我们知道，word2vec实际上就是一个神经网络（后面会解释），那么这样的数据，我们是以什么样的格式用来训练的呢？\n",
    "看一张图，你就明白了：\n",
    "\n",
    "![word2vec window](images/word2vec_window.png)\n",
    "\n",
    "可以看到，我们总是以**中间词**放在第一个位置，然后跟着我们的前后相邻词。可以看到，每一对词都是一个输入和一个输出组成的数据对。\n",
    "\n",
    "所以，我们训练模型之前，需要根据语料，整理出所有的像上面这样的输入数据用来训练。\n",
    "\n",
    "## 神经网络\n",
    "\n",
    "word2vec是一个简单的神经网络，有以下几个层组成：\n",
    "\n",
    "* １个输入层\n",
    "* 1个隐藏层\n",
    "* 1个输出层\n",
    "\n",
    "输入层输入的就是上面我们说的数据对的数字表示，输出到隐藏层。\n",
    "隐藏层的神经网络单元的数量，其实就是我们所说的**embedding size**，只有为什么，我们后面简单计算一下就知道。需要注意的是，我们的隐藏层后面不需要使用激活函数。\n",
    "输出层，我们使用softmax操作，得到每一个预测结果的概率。\n",
    "\n",
    "这里有一张图，能够大致表示这个网络：\n",
    "![skip-gram-net-arhc](images/skip_gram_net_arch.png)\n",
    "\n",
    "现在问题来了，刚刚我们说，输入层的输入是我们之前准备的数据对的数字表示，那么我们该如何用数字表示文本数据呢？\n",
    "\n",
    "好像随便一种方式都可以用来表示我们的文本啊。\n",
    "\n",
    "看上图，我们发现，它的输入使用的是**one-hot**编码。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
