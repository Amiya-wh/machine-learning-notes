{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Transformer学习笔记\n",
    "\n",
    "论文《Attention Is All You Need》提出了一个全新的完全基于Attention机制的Transformer模型，该模型不需要CNN或者RNN，性能突出，训练更快。\n",
    "\n",
    "论文地址： [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "\n",
    "GitHub已经有人实现了Transformer模型，比如： [Kyubyong/transformer](https://github.com/Kyubyong/transformer/blob/master/modules.py)\n",
    "\n",
    "当然，Google自己也有实现，见[tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)\n",
    "\n",
    "我是根据Kyubyong的实现来理解Transformer模型的。\n",
    "\n",
    "根据论文，Transformer模型的架构如下图：\n",
    "\n",
    "![transformer architecture](images/transformer.jpg)\n",
    "\n",
    "这张图的结构很清晰，Kyubyong的实现也是完全根据这张图的结构一步一步来的。那我们逐步就理解一下代码。\n",
    "\n",
    "## Normalize\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def normalize(inputs, epsilon=1e-8, scope=\"normalize\", reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 获取inputs的形状\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        # 参数的形状，是inputs形状的最后一行\n",
    "        # 这种情况有点类似于 f= xW + b 中，把b合并到W和x中，x最后一行加上全是1的向量，W中最后一列增加一列\n",
    "        params_shape = inputs_shape[-1:]\n",
    "        \n",
    "        # 求出inputs的均值和方差，两个张量的维度和inputs保持一致\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        \n",
    "        beta = tf.Variable(tf.zeros(params_shape))\n",
    "        gamma = tf.Variable(tf.ones(params_shape))\n",
    "        # 使用epsilon是为了防止数值计算错误\n",
    "        normalized = (inputs - mean) / ((variance + epsilon) ** (.5))\n",
    "        \n",
    "        # 经过线性计算，得到输出output\n",
    "        outputs = gamma * normalized + beta\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "词嵌入阶段，这个大家应该很熟悉了。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def embedding(inputs, vocab_size, num_units, zero_pad=True, scale=True, scope=\"embedding\", reuse=None):\n",
    "    \"\"\"词嵌入。\n",
    "    \n",
    "    Args:\n",
    "        inputs: 输入张量\n",
    "        vocab_size: 词典数量\n",
    "        num_units: 隐藏层单元数量，也就是常说的embedding_size\n",
    "        zero_pad: 是否增加一行全0的向量\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable(\"lookup_table\",\n",
    "                                      dtype=tf.float32,\n",
    "                                      shape=[vocab_size, num_units],\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            # 在lookup_table的第一行加上一行全是0的向量\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:,:]),0)\n",
    "        \n",
    "        output = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        if scale:\n",
    "            output = output * (num_units ** 0.5)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "什么是Positional encoding? 我一开始接触这个词的时候，一脸懵逼。因为传统的seq2seq好像没有这个概念，即使是带有Attention机制的seq2seq也没有这个概念。\n",
    "\n",
    "我们看懂代码，再来回答这个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def positional_encoding(inputs, num_units, zero_pad=True, scale=True, scope=\"positional_encoding\", reuse=None):\n",
    "    \"\"\"正弦位置编码。\n",
    "    \n",
    "    Args:\n",
    "        inputs: 输入，2维张量\n",
    "        num_units: 输出维度\n",
    "        zero_pad: 是否增加一行全0的向量\n",
    "        scale: 是否乘以num_units的平方根\n",
    "    \"\"\"\n",
    "    # N就是batch_size，T就是time_steps\n",
    "    N, T = inputs.get_shape().as_list()\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1])\n",
    "\n",
    "        # 根据论文的公式，得到一个二维张量\n",
    "        position_enc = np.array([\n",
    "            [pos / np.power(10000, 2.*i/num_units) for i in range(num_units)]\n",
    "            for pos in range(T)])\n",
    "\n",
    "        # 奇数列使用cosine计算，偶数列使用sin计算\n",
    "        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n",
    "        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n",
    "\n",
    "        # 转化成一个张量，通过position就可以查出对应的数值，这一点和embedding十分相似\n",
    "        lookup_table = tf.convert_to_tensor(position_enc)\n",
    "\n",
    "        if zero_pad:\n",
    "            # 在第一行加上一行全0的向量\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:, :]), 0)\n",
    "        # 根据位置，查表，获取输出\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)\n",
    "\n",
    "        if scale:\n",
    "            outputs = outputs * num_units**0.5\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
