{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Transformer的PyTorch实现\n",
    "\n",
    "> 本文由**罗周杨 stupidme.me.lzy@gmail.com**原创，转载请注明原作者和出处。\n",
    "> 本文严禁用于商业用途。\n",
    "\n",
    "之前研究了一番**Transformer**模型，也有一个笔记[Transformer学习笔记](transformer.ipynb)，现在我们用PyTorch实现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position encoding的实现\n",
    "再回顾一下什么是Position encoding。我们知道Transformer模型由于没有使用RNN，必须使用额外的手段来获取文本序列的顺序（或者说位置）信息。我们的Position encoding(PE)就是实现这个目标而出现的。\n",
    "\n",
    "Position encoding和Word embedding十分相似。Word embeddings是对词语的内容进行嵌入，而Position encoding是对词语的位置进行嵌入。所以，Position encoding还有个很流行的名字，叫做Position embedding。\n",
    "\n",
    "下面我们看代码吧，然后讲解代码，你就可以弄懂其中的所有细节了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        \"\"\"初始化。\n",
    "        \n",
    "        Args:\n",
    "            d_model: 一个标量。模型的维度，论文默认是512\n",
    "            max_seq_len: 一个标量。文本序列的最大长度\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # 根据论文给的公式，构造出PE矩阵\n",
    "        position_encoding = np.array([\n",
    "          [pos / np.pow(10000, 2.0 * (j // 2) / d_model) for j in range(d_model)]\n",
    "          for pos in range(max_seq_len)])\n",
    "        # 偶数列使用sin，奇数列使用cos\n",
    "        position_encoding[:, 0::2] = np.sin(position_encoding[:, 0::2])\n",
    "        position_encoding[:, 1::2] = np.cos(position_encoding[:, 1::2])\n",
    "\n",
    "        # 在PE矩阵的第一行，加上一行全是0的向量，代表这`PAD`的positional encoding\n",
    "        # 在word embedding中也经常会加上`UNK`，代表位置单词的word embedding，两者十分类似\n",
    "        # 那么为什么需要这个额外的PAD的编码呢？很简单，因为文本序列的长度不一，我们需要对齐，\n",
    "        # 短的序列我们使用0在结尾补全，我们也需要这些补全位置的编码，也就是`PAD`对应的位置编码\n",
    "        pad_row = torch.zeros([1, d_model])\n",
    "        position_encoding = torch.cat((pad_row, position_encoding))\n",
    "        \n",
    "        # 嵌入操作，+1是因为增加了`PAD`这个补全位置的编码，\n",
    "        # Word embedding中如果词典增加`UNK`，我们也需要+1。看吧，两者十分相似\n",
    "        self.position_encoding = nn.Embedding(max_seq_len + 1, d_model)\n",
    "        self.position_encoding.weight = nn.Parameter(position_encoding,\n",
    "                                                     requires_grad=False)\n",
    "    def forward(self, input_len):\n",
    "        \"\"\"神经网络的前向传播。\n",
    "\n",
    "        Args:\n",
    "          input_len: 一个张量，形状为[BATCH_SIZE, 1]。每一个张量的值代表这一批文本序列中对应的长度。\n",
    "\n",
    "        Returns:\n",
    "          返回这一批序列的位置编码，进行了对齐。\n",
    "        \"\"\"\n",
    "        \n",
    "        # 找出这一批序列的最大长度\n",
    "        max_len = torch.max(input_len)\n",
    "        tensor = torch.cuda.LongTensor if input_len.is_cuda else torch.LongTensor\n",
    "        # 对每一个序列的位置进行对齐，在原序列位置的后面补上0\n",
    "        # 这里range从1开始也是因为要避开PAD(0)的位置\n",
    "        input_pos = tensor(\n",
    "          [list(range(1, len + 1)) + [0] * (max_len - len) for len in input_len])\n",
    "        return self.position_encoding(input_pos)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer normalization的实现\n",
    "Layer nomalization和Batch normalization的比较，在文章开头提到的笔记已经有说明了。\n",
    "\n",
    "接下来就来实现它吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"实现LayerNorm。其实PyTorch已经实现啦，见nn.LayerNorm。\"\"\"\n",
    "\n",
    "    def __init__(self, features, epsilon=1e-6):\n",
    "    \"\"\"Init.\n",
    "\n",
    "    Args:\n",
    "        features: 就是模型的维度。论文默认512\n",
    "        epsilon: 一个很小的数，防止数值计算的除0错误\n",
    "    \"\"\"\n",
    "    super(LayerNorm, self).__init__()\n",
    "    # weights\n",
    "    self.gamma = nn.Parameter(torch.ones(features))\n",
    "    # bias\n",
    "    self.beta = nn.Parameter(torch.zeros(features))\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "    \"\"\"Forward pass.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor, with shape of [B, L, D]\n",
    "    \"\"\"\n",
    "    # 根据公式进行归一化\n",
    "    # 在X的最后一个维度求均值，最后一个维度就是模型的维度\n",
    "    mean = x.mean(-1, keepdim=True)\n",
    "    # 在X的最后一个维度求方差，最后一个维度就是模型的维度\n",
    "    std = x.std(-1, keepdim=True)\n",
    "    return self.gamma * (x - mean) / (std + self.epsilon) + self.beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention的实现\n",
    "\n",
    "具体的原理在另一个笔记有提到。咱们现在实现它，这个代码也比较简单，根据论文的公式即可。唯一要注意的是，有个mask。\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, attention_dropout=0.0, scale=True):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "    \"\"\"Forward pass.\n",
    "\n",
    "    Args:\n",
    "      q: Queries tensor, with shape of [B, L_q, D_q]\n",
    "      k: Keys tensor, with shape of [B, L_k, D_k]\n",
    "      v: Values tensor, with shape of [B, L_v, D_v]\n",
    "      mask: A ByteTensor, binary mask. If not None, do mask.\n",
    "    \"\"\"\n",
    "        attention = torch.bmm(q, k.transpose(1, 2))\n",
    "        if self.scale:\n",
    "            d_k = k.size(-1)  # get model's dimension or num_units\n",
    "            attention = attention / np.sqrt(d_k)\n",
    "        if mask:\n",
    "            attention = attention.masked_fill_(mask, -np.inf)\n",
    "        attention = self.softmax(attention)\n",
    "        attention = self.dropout(attention)\n",
    "        context = torch.bmm(attention, v)\n",
    "        return context, attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
